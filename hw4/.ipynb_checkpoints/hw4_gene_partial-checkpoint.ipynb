{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression for Gene Expression Data\n",
    "\n",
    "In this exercise, we use logistic regression to predict biological characteristics (\"phenotypes\") from gene expression data. In doing this exercise, you will learn to:\n",
    "* Handle missing data\n",
    "* Perform multi-class logistic classification\n",
    "* Create a confusion matrix (a table of the fraction of samples of class i being confused with class j)\n",
    "* Use L1-regularization for improved estimation in the case of sparse weights \n",
    "\n",
    "## Background\n",
    "\n",
    "Genes are the basic unit in the DNA and encode blueprints for proteins.  When proteins are synthesized from a gene, the gene is said to \"express\".  Micro-arrays are devices that measure the expression levels of large numbers of genes in parallel.  By finding correlations between expression levels and phenotypes, scientists can identify possible genetic markers for biological characteristics.\n",
    "\n",
    "The data in this lab comes from:\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Mice+Protein+Expression\n",
    "\n",
    "In this data, mice were characterized by three properties:\n",
    "* Whether they had down's syndrome (trisomy) or not\n",
    "* Whether they were stimulated to learn or not\n",
    "* Whether they had a drug memantine or a saline control solution.\n",
    "\n",
    "With these three choices, there are 8 possible classes for each mouse.  For each mouse, the expression levels were measured across 77 genes.  We will see if the characteristics can be predicted from the gene expression levels.  This classification could reveal which genes are potentially involved in Down's syndrome and if drugs and learning have any noticeable effects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "We begin by loading the standard modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn import linear_model, preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `pd.read_excel` command to read the data from \n",
    "\n",
    "https://archive.ics.uci.edu/ml/machine-learning-databases/00342/Data_Cortex_Nuclear.xls\n",
    "\n",
    "into a dataframe `df`.  Use the `index_col` option to specify that column 0 is the index.  Use the `df.head()` to print the first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "df = pd.read_excel(\"/Users/caesar/Dropbox/C261/Data_Cortex_Nuclear.xls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data has missing values.  The site:\n",
    "\n",
    "http://pandas.pydata.org/pandas-docs/stable/missing_data.html\n",
    "\n",
    "has an excellent summary of methods to deal with missing values.  Following the techniques there, create a new data frame `df1` where the missing values in each column are filled with the mean values from the non-missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "df1 = df.fillna(df.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification for Down's Syndrome\n",
    "\n",
    "We will first predict the binary class label in `df1['Genotype']` which indicates if the mouse has Down's syndrome or not.  Get the string values in `df1['Genotype'].values` and convert this to a numeric vector `y` with 0 or 1.  You may wish to use the `np.unique` command with the `return_inverse=True` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "vals, y = np.unique(df1['Genotype'], return_inverse = True)\n",
    "\n",
    "vals = np.array([0, 1])\n",
    "\n",
    "df1['Genotype'] = pd.Series(vals[y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the predictors, we will use all but the last four columns of the dataframes.  First, get the names of these genes with the command:\n",
    "\n",
    "    xnames = df1.columns[:-4]\n",
    "    \n",
    "Then, get the data values corresponding to these columns and store the values in a matrix `X`.  This matrix will have the expression levels for the 77 genes.  Standardize the data matrix and call the standardized matrix `Xs`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Get names of the genes\n",
    "xnames=df1.columns[1:-4]\n",
    "\n",
    "# TODO\n",
    "X = df1[xnames]\n",
    "Xs = preprocessing.scale(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `LogisticRegression` object `logreg` and `fit` the training data.  You can leave the arguments blank or specify `C=1`; those are equivalent.  `C` is an inverse L2 regularization constant.  Changing it changes the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "logreg = linear_model.LogisticRegression(C=1)\n",
    "logreg.fit(Xs, vals[y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the accuracy of the classifer.  That is, use the `logreg.predict` function to predict labels `yhat` and measure the fraction of time that the predictions match the true labels.  Below, we will properly measure the accuracy on cross-validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of true predictions:  0.985\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "yhat = logreg.predict(Xs)\n",
    "ys = vals[y]\n",
    "acc = sum(yhat == ys)\n",
    "print(\"Fraction of true predictions: \", round(acc/len(yhat), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the weight vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a stem plot of the coefficients, `W` in the logistic regression model.  You can get the coefficients from `logreg.coef_`, but you will need to reshape this to a 1D array.  (Unlike in R or matlab, in Python, an `n`-length row or column vector is considered to be of shape or size `n`.  It is a 1-dimensional object, not an `nx1` or `1xn` object.  So a 2D array consisting of only 1 row or 1 column is not the same thing as being a 1D array.  You can use the `ravel` method where if `u` is `1xn`, `u.ravel()` would return a 1d version of `u`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Coeff values')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO\n",
    "W = logreg.coef_\n",
    "W = np.ravel(W)\n",
    "# Plot W\n",
    "plt.figure()\n",
    "plt.stem(W)\n",
    "plt.title(\"Regression Coeffs Stem Plot\")\n",
    "plt.xlabel(\"Coefficients\")\n",
    "plt.ylabel(\"Coeff values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that `W[i]` is very large for a few components `i`.  These are the genes that are likely to be most involved in Down's Syndrome.  Although, we do not discuss it in this class, there are ways to force the logistic regression to return a sparse vector `W`.  \n",
    "\n",
    "Find the names of the genes for two components `i` where the magnitude of `W[i]` is largest. You could use the command np.argsort with the appropriate arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two most invovled genes components:  [24 23]\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "large = np.argsort(W)\n",
    "print(\"Two most invovled genes components: \", large[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "The above meaured the accuracy on the training data.  It is better to measure the accuracy on the test data.  Perform 10-fold cross validation and measure the accuracy.  Note, that in performing the cross-validation, you will want to randomly permute the test and training sets using the `shuffle` option.  In this data set, all the samples from each class are bunched together, so shuffling is essential.  Print the mean accuracy across all the folds.  (If you already did the homework and found precision, f1-score and recall, that is fine.  You do not have to redo this slightly easier version. If you want to the latter, you will need to import precision_recall_fscore_support from sklearn.metrics.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy across all folds =  0.972\n"
     ]
    }
   ],
   "source": [
    "# TODO A possible beginning here\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# This line is not necesssary if you are just finding accuracy\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "nfold = 10\n",
    "kf = KFold(n_splits=nfold, random_state = 1,shuffle=True)\n",
    "\n",
    "# Create an empty list acc with desired length, for memory efficiency\n",
    "acc = [None]*nfold\n",
    "\n",
    "for i, v in enumerate(kf.split(X)):\n",
    "    Xtr = Xs[v[0].tolist()]\n",
    "    Xts = Xs[v[1].tolist()]\n",
    "    ytr = ys[v[0].tolist()]\n",
    "    yts = ys[v[1].tolist()]\n",
    "    \n",
    "    # Calculate and store all accuracies\n",
    "    logreg.fit(Xtr, ytr)\n",
    "    yhati = logreg.predict(Xts)\n",
    "    # Store the fractions directly in acc\n",
    "    acc[i] = sum(yhati == yts)/len(yts)\n",
    "\n",
    "# Print results\n",
    "print(\"Mean accuracy across all folds = \", round(np.mean(acc), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Class Classification\n",
    "\n",
    "Now use the response variable in `df1['class']`.  This data has 8 possible classes from the three binary properties of mice described above.  Use the `np.unique` funtion as before to convert this to a vector `y` with values 0 to 7. You can see them by starting below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "ystr = df1['class'].values\n",
    "# TODO\n",
    "vals, y = np.unique(ystr, return_inverse = True)\n",
    "\n",
    "vals2 = np.array(list(range(8)))\n",
    "\n",
    "df1['class'] = pd.Series(vals2[y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a multi-class logistic model by creating a `LogisticRegression` object, `logreg` and then calling the `logreg.fit` method.  Again, use C=1 or leave the arguments blank.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the accuracy on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform 10-fold cross validation, and measure the confusion matrix `C` on the test data in each fold. You can use the `confusion_matrix` method in the `sklearn` package.  Add the confusion matrix counts across all folds and then normalize the rows of the confusion matrix so that they sum to one.  Thus, each element `C[i,j]` will represent the fraction of samples where `yhat==j` given `ytrue==i`.  Print the confusion matrix.  You can use the command\n",
    "\n",
    "    print(np.array_str(C, precision=4, suppress_small=True))\n",
    "    \n",
    "to create a nicely formatted print.  Also print the overall mean and SE of the test accuracy across the folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO  POssible beginning\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "logreg = linear_model.LogisticRegression()\n",
    "\n",
    "# Initialize the confusion matrix counts\n",
    "ny = np.max(y)\n",
    "C = np.zeros((ny+1,ny+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-run the logistic regression on the entire training data and get the weight coefficients.  This should be a 8 x 77 matrix.  Create a stem plot of the first row of this matrix to see the coefficients on each of the genes for predicting the first class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## L1-Regularization\n",
    "\n",
    "\n",
    "In most genetic problems, only a limited number of the tested genes are likely influence any particular attribute.  Hence, we would expect that the weight coefficients in the logistic regression model should be sparse.  That is, they should be zero on any gene that plays no role in the particular attribute of interest.  Genetic analysis commonly imposes sparsity by adding an l1-penalty term.  Read the `sklearn` [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) on the `LogisticRegression` class to see how to set the l1-penalty and the inverse regularization strength, `C`.\n",
    "\n",
    "Use K-fold cross validation to select an appropriate inverse regularization strength.  \n",
    "* Use 10-fold cross validation \n",
    "* You should select around 20 values of `C`.  It is up to you find a good range.\n",
    "* Make appropriate plots and print out to display your results\n",
    "* Use the one  SE rule to find the optimal `C`.\n",
    "* How does the accuracy compare to the accuracy achieved without regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO:  Perform the cross validation to compute the accuracy per fold and per C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO:  Compute the mean and one SE accuracy and plot with errorbars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO:  Compute the optimal C with the one SE rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the optimal `C`, fit the model on the entire training data with l1 regularization. Find the resulting weight matrix, `W_l1`.  Plot the first row of this weight matrix and compare it to the first row of the weight matrix without the regularization.  You should see that, with l1-regularization, the weight matrix is much more sparse and hence the roles of particular genes are more clearly visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
